## 제4장: Quick, Draw! 그림 인식

### 개요
이 장에서는 인기 게임 Quick, Draw!의 그림을 인식하는 기술을 배우게 됩니다.

### 자료
- **블로그 포스트**: 분석과 기술에 대한 자세한 내용은 [이 블로그 포스트](https://velog.io/@thdfydgh/Kaggle-Quick-Draw)에서 확인하세요.
- **대회 링크**: 직접 도전해 보고 싶다면 [카글 대회 페이지](https://www.kaggle.com/competitions/quickdraw-doodle-recognition)를 방문하세요.
- **참고 코드**: 실제 구현과 팁을 위해 [이 카글 커널](https://www.kaggle.com/code/ttagu99/train-model)을 탐색하세요.

### 목표
> 1. 대용량 이미지 데이터를 관리하는 방법을 숙달한다. (DataBase) <br>
> 2. 컴퓨터 자원(메모리, GPU, CPU) 고려하여 적절하게 전처리한 스킬을 숙달한다.
> 3. 얻은 인사이트를 정리한다.
---

#### 인사이트1 : 대회 평가지표를 함수로 구현해라: Validation 활용
```python
def apk(actual, predicted, k):
    if actual not in predicted:  # 정답이 제출한 값에 없으면 score는 0
        return 0.0
    for i in range(k):
        if actual in  predicted[:i+1]:  # 제출한 값이 K번째 만에 정답
            return 1.0 / len(predicted[:i+1])  # score
```

보통 리더보드에  submission파일을 올려 점수를 리더보드에서 확인하는 대회같은 경우, 난 단순하게 대회에서 채점하는걸 기준으로 삼았다.

직점 함수를 구현하여, 평가지표를 깊이 이해하고 채점되는 원리를 이해한다면, 모델링이나 전처리 부분에서 더 나은 선택에 활용 할 수 있다.

### EDA
---
### DataAugmentation
---
#### 인사이트 : 데이터 증강 기법은 점수향상에 큰 영향을 미친다
창의적인 증강 기법을 찾아 추가하는데 노력을 많이 기울이게 된다. 이 대회는 데이터 수가 5000만개나 되어서 1epoch 학습하는데만 하루 넘게 걸린다. 데이터 증강이 필요 없어 보여도, 최소한의 방법으로 어떻게든 적용을 시킨다.


1. 획 중 나중에 그려진 획들을 랜덤하게 삭제 : 어차피 나중에 그려진건 쓸모가 없다
(CPU 부하에 아무런 영향이 없다)
2. 두 종류의 그림을 동시에 그리기 : 두 가지가 섞여 있을 때, 두 레이블에 대해 예측 확률이 비슷하게 나온 모델이 더 잘 학습된 모델 (?)
( 객체 위치가 달라져 Random Crop 증강 기법 효과가 있다)
CPU 부하가 있지만 다른 기법들에 비하면 부하량은 매우 적다.

>"사실 2번은 잘 와닿지 않는다"

점검사항 만들기
1. 이미지와 레이블이 잘 연결 되었는가?
2. 훈련 데이터 순서가 랜덤하게 섞여 있나?
3. 증강 기법 적용 결과가 실제에서 나올법 한가?

### 모델링
#### 인사이트2 : KFold 못쓰니까 K=1 : OOF(out of Fold) 사용
홀드아웃이라고도 부른다.
학습 모델이 여러개일 경우, 사용할 val셋과 train셋은 바꾼다.
: 데이터 셋이 작으면 cross valid 그렇게 하겠는데 이 대회는 교차 검증 하는게 하루가 걸리기 때문.
>학습 모델은 서로 다른 모델을 몇개 학습할 것인데, 이때 사용할 Validation Set을 바꾸고, Train Set도 파일 1개는 바뀌게 할 것입니다. 이렇게 한 이유는 Data Set이 작다면 (학습하는데 몇시간 정도라면) 1개의 모델에 대해서도 Cross Validation 하는게 일반적인 방법이지만, 이 대회의 경우는 1epoch가 작은 모델도 하루가 걸리기 때문에 4개 정도의 모델을 2epoch 정도 만 학습할 것이고, 이때 각기 서로 다른 Train-Validation Set를 사용하겠습니다. 모델도 다르지만, 학습하는 Data Set도 약간 다르기 때문에 각기 모델이 같은 Data Set으로 학습하는 것보다는 서로 상관도가 적을 것입니다. ensemble 및 stacking model은 base가 되는 모델이 서로 상관도가 적을 때 높은 score 이득을 얻을 수가 있습니다.

#### 인사이트3 : Hyperparameter 조정
>학습하는 메인 코드는 다음과 같습니다. 먼저 정의한 모델들에 대해 for loop를 돌며 학습하고, 학습이 완료된 후에는 hold out set과 test set의 probability를 저장합니다. 이때 Learning Rate는 특별 히 Search 하지 않고 default Learning Rate 인 0.001을 사용했고, validation 시 마다 MAP@3 score값이 증가하면 weight save를 하였고, 10 virtual epoch 동안 score가 증가하지 않으면 Learning Rate를 0.5 곱해 주었습니다. 다른 딥러닝 모델들을 학습 해 보신 경험이 있으시다면, Learning Rate 감소가 너무 보수 적이게 되어 있다고 느낄 수 있습니다. 하지만 지금 validation 하는 주기가 1 virtual epoch(실제 Epoch의 1/100 수준)이기 때문에 오히려 빠르게 감소하게 만든 것입니다. 그리고 학습하기 전에 train file 리스트를 한번 shuffle하는 이유는 1개 모델을 1 real epoch 학습하는데 걸리는 시간이 24시간~48시간 이기 때문에, 다른 컴퓨터 사용이나, 예기치 않게 학습이 중단될 경우 파일 리스트를 같은 순서로 학습해서 overfitting 되는 문제를 완화하기 위한 것입니다. 이 코드를 수행하면 계획한 모델들에 대해 각각 probability, submission 파일 들이 저장됩니다. 


lr을 줄인다..? 그게 더 빠르게 감소..?



#### 인사이트4 : Submission
> 대회 중에는 미처 알지 못했지만, 대회가 끝난 후 1등 팀이 solution을 공개했을 때 test set의 분포에 magic이 있었음이 알려 졌습니다. _**카테고리 별 예측의 수를 고르게 balancing해서 하면 score가 올라간다는 것입니다. 거기에 대한 hint는 test set의 개수에서 포착되었습니다. test set이 112199개로, label 숫자인 340개로 나누면 나머지가 339개로 1개 부족합니다. test set의 label 분포가 고르게 있을 것이라는 의심이 들게 됩니다.**_ 그런데 이런 magic이 이 대회 한번만 있었던 일은 아닙니다. test set의 분포를 알 수 있는 hint가 어느 정도 있다면, 최종 제출 파일을 2개 선택할 수 있기 때문에, 하나 정도는 overfitting을 감안하고서 선택할 수도 있겠습니다. 2개를 모두 같은 방식으로 하는 것은 test set 분포가 예측과 다를 때에는 망할 수 있기 때문에 둘 중 하나는 정상적인 결과 물을, 하나는 test set 분포를 활용해 post processing된 결과물을 제출하겠습니다.

좀 비겁하지만 대회 우승에 비겁이 뭐가 중요한가? Data leakage한 대회측 잘못이지. 한 번쯤은 시험지를 뚫어져라 쳐다 보는 것도 방법일듯.

다른 제출자는 40개 CNN을 앙상블 했다. 이정도면 광기 아닌가?






#### 인사이트 5 : 대회 복기
>저는 지금까지 한번도 끝난 대회를 복기하려 하지 않았고, 캐글 할 시간이 조금이라도 나면 새로운 대회를 해야겠다라는 마음만 가지고 있었습니다. 이 책을 쓰며 그것이 잘못된 생각이 었다는 것을 깨달았습니다. 나 자신이 만든 코드에, 많은 오류가 있었고, 그 오류를 만들지 않았다면 훨씬 더 좋은 점수를 낼 수 있었다는 것을 알았습니다. 하지만 실수 했던 오류들을 복기 하지 않았으니, 비슷한 대회에서 같은 오류를 범하고 있었습니다. 들어간 시간에 비해 실력이 많이 안늘었겠구나 라는 생각이 듭니다. 이전에 캐글하는 시간 분배가 새로운 대회:끝난 대회 10:0 이었다면, 9:1 정도는 배분해서 끝난 대회를 복기하며 하나의 대회를 마무리 해야겠습니다.

도파민에 절여지지 말자.
